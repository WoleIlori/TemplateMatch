{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TemplateMatch",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "N9rh8IRZOvGA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn import metrics\n",
        "from scipy.spatial import distance\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D93Hg_AvSj_F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Implementing my own classifier which creates a template for each class in target column\n",
        "#Each template stores the average value of each feature for that class\n",
        "#The classifier will select the template that is similar to the query using a distance function\n",
        "\n",
        "class TemplateMatch(BaseEstimator):\n",
        "    #step 1: get all the target levels\n",
        "    #step 2: get the average of the instance for each level\n",
        "    #step 3: store it as a template for each level\n",
        "    def __init__(self, dist_metric = 'euclidean'):\n",
        "        self.dist_metric = dist_metric\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \n",
        "        X, y = check_X_y(X, y)\n",
        "        \n",
        "        #get each target level\n",
        "        unique = np.unique(y)\n",
        "        self.classes_ = unique\n",
        "        \n",
        "        #append the target column to the data\n",
        "        X = np.c_[X, y]\n",
        "        \n",
        "        #This is to find all the instances related to each target class\n",
        "        #convert to dataframe for readability\n",
        "        X = pd.DataFrame(X)\n",
        "        \n",
        "        #index for target column\n",
        "        target = len(X.columns) -1\n",
        "        \n",
        "        #store avg of each column per row\n",
        "        rows_list = []\n",
        "        \n",
        "        for i in unique:\n",
        "          row = []\n",
        "      \n",
        "          #get all the instances that contains i class\n",
        "          instances = X[X[target] == i]\n",
        "          \n",
        "          #calculate the average for each column in i class\n",
        "          for j in X.columns:\n",
        "            if( j != target):\n",
        "              col_sum = sum(instances[j])\n",
        "              avg = col_sum / len(instances)\n",
        "              row.append(avg)\n",
        "          rows_list.append(row)\n",
        "          \n",
        "        self.template_ = np.array(rows_list)\n",
        "       \n",
        "        return self\n",
        "    \n",
        "    #find the template that is most similar to the query instance using euclidean\n",
        "    def predict_euclidean(self, X):\n",
        "      \n",
        "      check_is_fitted(self, ['template_'])\n",
        "      \n",
        "      X = check_array(X)\n",
        "      predictions = []\n",
        "      \n",
        "      for instance in X:\n",
        "        #first assign the target class 0 as the template similar to the query\n",
        "        match = len(X) - 1\n",
        "        dist = distance.euclidean(instance, self.template_[0])\n",
        "        for i in range(10):\n",
        "          if( distance.euclidean(instance, self.template_[i]) < dist):\n",
        "            match = i\n",
        "            dist = distance.euclidean(instance, self.template_[i])\n",
        "        predictions.append(match)\n",
        "      \n",
        "      return predictions\n",
        "    \n",
        "    #find the template that is most similar to the query instance using manhattan\n",
        "    def predict_manhattan(self, X):\n",
        "      check_is_fitted(self, ['template_'])\n",
        "      \n",
        "      X = check_array(X)\n",
        "      predictions = []\n",
        "      \n",
        "      for instance in X:\n",
        "        #first assign the target class 0 as the template similar to the query\n",
        "        match = 0\n",
        "        dist = distance.cityblock(instance, self.template_[0])\n",
        "        for i in range(10):\n",
        "          if( distance.cityblock(instance, self.template_[i]) < dist):\n",
        "            match = i\n",
        "            dist = distance.cityblock(instance, self.template_[i])\n",
        "        predictions.append(match)\n",
        "      \n",
        "      return predictions\n",
        "    \n",
        "    def predict_chebyshev(self, X):\n",
        "      check_is_fitted(self, ['template_'])\n",
        "      \n",
        "      X = check_array(X)\n",
        "      predictions = []\n",
        "      \n",
        "      for instance in X:\n",
        "        #first assign the target class 0 as the template similar to the query\n",
        "        match = 0\n",
        "        dist = distance.chebyshev(instance, self.template_[0])\n",
        "        for i in range(10):\n",
        "          if( distance.chebyshev(instance, self.template_[i]) < dist):\n",
        "            match = i\n",
        "            dist = distance.cityblock(instance, self.template_[i])\n",
        "        predictions.append(match)\n",
        "      \n",
        "      return predictions\n",
        "    \n",
        "    #To be used with GridSearch\n",
        "    def predict(self, X):\n",
        "      check_is_fitted(self, ['template_'])\n",
        "      \n",
        "      X = check_array(X)\n",
        "      predictions = []\n",
        "      \n",
        "      if(self.dist_metric == 'euclidean'):\n",
        "        dist_type = self.getEuclidDistance\n",
        "      elif(self.dist_metric == 'manhattan'):\n",
        "        dist_type = self.getManhattanDistance\n",
        "      elif(self.dist_metric == 'chebyshev'):\n",
        "        dist_type = self.getChebyshevDistance\n",
        "      \n",
        "      for instance in X:\n",
        "        #first assign the target class 0 as the template similar to the query\n",
        "        match = 0\n",
        "        dist = dist_type(instance, self.template_[0])\n",
        "        for i in range(10):\n",
        "          if(dist_type(instance, self.template_[i]) < dist):\n",
        "            match = i\n",
        "            dist = dist_type(instance, self.template_[i])\n",
        "        predictions.append(match)\n",
        "      \n",
        "      return predictions\n",
        "    \n",
        "    \n",
        "    def getEuclidDistance(self,x, y):\n",
        "      return distance.euclidean(x, y)\n",
        "    \n",
        "    def getManhattanDistance(self,x, y):\n",
        "      return distance.cityblock(x, y)\n",
        "    \n",
        "    def getChebyshevDistance(self,x, y):\n",
        "      return distance.chebyshev(x, y)\n",
        "        \n",
        "       \n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ySzIsYaBqIWT",
        "colab_type": "code",
        "outputId": "24ec90b4-a074-4a10-c9f0-73f8d18ee545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6_rYpBD1qZGq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/drive/My Drive/Dataset/fashion-mnist_train.csv')\n",
        "test_data = pd.read_csv('/content/drive/My Drive/Dataset/fashion-mnist_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kCppKPDJerJb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "1rAbgFE5QRYL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "target = \"label\"\n",
        "y = train_data[\"label\"]\n",
        "X =train_data[train_data.columns.difference([\"label\"])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aFPzLAkrqM7E",
        "colab_type": "code",
        "outputId": "c2b51bbd-d3f1-4dfa-e81f-90fcea9b917b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "my_model = TemplateMatch()\n",
        "my_model.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TemplateMatch(dist_metric='euclidean')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "Cw38-UqgUNKN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Find the optimal distance metric using GrideSearch CV\n",
        "param_grid = [\n",
        "    {'dist_metric':['euclidean', 'manhattan', 'chebyshev']}\n",
        "]\n",
        "\n",
        "my_tuned_model = GridSearchCV(TemplateMatch(), param_grid, cv=2, verbose = 0,\\\n",
        "                              scoring = metrics.make_scorer(metrics.accuracy_score))\n",
        "\n",
        "my_tuned_model.fit(X, y)\n",
        "\n",
        "# Print details\n",
        "print(\"Best parameters set found on development set:\")\n",
        "print(my_tuned_model.best_params_)\n",
        "print(my_tuned_model.best_score_)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mPtjtiKWbxGe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#creae a sample of test_data for fast testing\n",
        "data_sampling_rate = 0.1\n",
        "test_data = test_data.sample(frac=data_sampling_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yB-S5fHDX4u4",
        "colab_type": "code",
        "outputId": "3184d70a-996e-4257-daac-cf38d41057fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "#predicting using euclidean distance\n",
        "X_test = test_data[test_data.columns.difference([\"label\"])]\n",
        "y_test = np.array(test_data[\"label\"])\n",
        "y_pred = my_model.predict_euclidean(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: \" + str(accuracy))\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.225\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       110\n",
            "           1       0.00      0.00      0.00       101\n",
            "           2       0.29      0.08      0.12        93\n",
            "           3       0.00      0.00      0.00       103\n",
            "           4       0.00      0.00      0.00       108\n",
            "           5       0.18      0.93      0.30        90\n",
            "           6       0.21      0.27      0.24       109\n",
            "           7       0.57      0.44      0.50        89\n",
            "           8       0.22      0.72      0.33        92\n",
            "           9       0.00      0.00      0.00       105\n",
            "\n",
            "   micro avg       0.23      0.23      0.23      1000\n",
            "   macro avg       0.15      0.24      0.15      1000\n",
            "weighted avg       0.14      0.23      0.14      1000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "00wUGKP_SKSu",
        "colab_type": "code",
        "outputId": "14d0a1f2-e6df-4b73-dcfa-e4bcb2a06612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "#predicting using manhattan\n",
        "X_test = test_data[test_data.columns.difference([\"label\"])]\n",
        "y_test = np.array(test_data[\"label\"])\n",
        "y_pred = my_model.predict_manhattan(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: \" + str(accuracy))\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.283\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.27      0.41        98\n",
            "           1       0.00      0.00      0.00       102\n",
            "           2       0.26      0.19      0.22        85\n",
            "           3       0.00      0.00      0.00       100\n",
            "           4       0.32      0.17      0.22       113\n",
            "           5       0.14      0.61      0.22       101\n",
            "           6       0.29      0.17      0.22        98\n",
            "           7       0.41      0.87      0.55       100\n",
            "           8       0.69      0.34      0.46       100\n",
            "           9       0.33      0.21      0.26       103\n",
            "\n",
            "   micro avg       0.28      0.28      0.28      1000\n",
            "   macro avg       0.33      0.28      0.26      1000\n",
            "weighted avg       0.33      0.28      0.26      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GB8IPBOVUDPe",
        "colab_type": "code",
        "outputId": "e4f852ad-c26f-4886-e770-f5984897711e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "#predicting using chebyshev\n",
        "X_test = test_data[test_data.columns.difference([\"label\"])]\n",
        "y = np.array(test_data[\"label\"])\n",
        "y_pred = my_model.predict_chebyshev(X_test)\n",
        "accuracy = metrics.accuracy_score(y, y_pred)\n",
        "print(\"Accuracy: \" + str(accuracy))\n",
        "print(metrics.classification_report(y, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.111\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.09      0.14        98\n",
            "           1       0.00      0.00      0.00       102\n",
            "           2       0.00      0.00      0.00        85\n",
            "           3       0.00      0.00      0.00       100\n",
            "           4       0.00      0.00      0.00       113\n",
            "           5       0.00      0.00      0.00       101\n",
            "           6       0.00      0.00      0.00        98\n",
            "           7       0.00      0.00      0.00       100\n",
            "           8       0.00      0.00      0.00       100\n",
            "           9       0.11      0.99      0.19       103\n",
            "\n",
            "   micro avg       0.11      0.11      0.11      1000\n",
            "   macro avg       0.04      0.11      0.03      1000\n",
            "weighted avg       0.04      0.11      0.03      1000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}